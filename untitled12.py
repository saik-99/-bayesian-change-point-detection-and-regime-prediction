# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mxf1zh-iXDDjAVmjoO2A4IrvW27sbpTA
"""

pip install pymc arviz numpy pandas matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Set a seed for reproducibility
np.random.seed(42)

# Generate synthetic data
X = np.linspace(0, 10, 100)
true_beta_0 = 1.5  # True intercept
true_beta_1 = 2.5  # True slope
true_sigma = 1.0   # True standard deviation of the error

# Calculate y with some random noise
y = true_beta_0 + true_beta_1 * X + np.random.normal(0, true_sigma, 100)

# Create a pandas DataFrame
df = pd.DataFrame({'X': X, 'y': y})

# Plot the data to see the relationship
plt.scatter(df['X'], df['y'])
plt.xlabel('X')
plt.ylabel('y')
plt.title('Synthetic Data for Linear Regression')
plt.show()

import pymc as pm

with pm.Model() as bayesian_linear_model:
    # 1. Define Priors for the model parameters
    # We'll use non-informative (vague) priors since we don't have strong prior beliefs.

    # Prior for the intercept (beta_0) - A Normal distribution
    beta_0 = pm.Normal('beta_0', mu=0, sigma=10)

    # Prior for the slope (beta_1) - A Normal distribution
    beta_1 = pm.Normal('beta_1', mu=0, sigma=10)

    # Prior for the error standard deviation (sigma) - A Half-Normal distribution
    # (since standard deviation must be positive)
    sigma = pm.HalfNormal('sigma', sigma=5)

    # 2. Define the Likelihood of the data
    # This is the linear model that connects our parameters to the observed data.
    # The expected value (mu) of y is our linear equation.
    mu = beta_0 + beta_1 * df['X']

    # The likelihood is a Normal distribution, with our linear model as the mean.
    # This tells PyMC that our observed data (y) is expected to be normally distributed
    # around the mean (mu) with a standard deviation of sigma.
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=df['y'])

    # 3. Fit the model using MCMC
    # This is the step where PyMC finds the posterior distributions.
    trace = pm.sample(2000, tune=1000, cores=1)

import arviz as az

# Plot the posterior distributions and trace plots
az.plot_trace(trace)
plt.show()

# Get a summary of the posterior distributions
summary = az.summary(trace)
print(summary)

import pymc as pm
import arviz as az
import matplotlib.pyplot as plt
import numpy as np

# This assumes 'bayesian_linear_model', 'trace', 'X', and 'y' already exist from the previous steps.

# 1. Generate posterior predictive samples
with bayesian_linear_model:
    # We ask PyMC to generate new 'y' values based on the posterior distributions of our parameters
    posterior_predictive = pm.sample_posterior_predictive(trace, random_seed=42)

# 2. Plot the results
plt.figure(figsize=(10, 6))

# Plot the original data points
plt.scatter(X, y, color='blue', label='Observed Data', alpha=0.6)

# Get the posterior predictive samples for our observed variable 'y_obs'
# The result from sample_posterior_predictive is an xarray object. We extract the values.
y_pred_samples = posterior_predictive.posterior_predictive["y_obs"]

# ArviZ's plot_hdi is perfect for plotting the mean prediction and its credible interval (HDI)
# We plot the HDI for the *expected value* of y.
az.plot_hdi(
    X,
    y_pred_samples,
    hdi_prob=0.94,  # This plots the 94% Highest Density Interval
    color='red',
    fill_kwargs={'alpha': 0.2, 'label': '94% HDI for Predictions'},
    ax=plt.gca() # Use the current axes
)

# You can also plot a number of individual regression lines to see the variety of fits
# First, reshape the data for easier plotting
y_preds_reshaped = y_pred_samples.stack(sample=("chain", "draw")).values
# Select a random subset of 100 lines to plot
indices = np.random.randint(0, y_preds_reshaped.shape[1], 100)
plt.plot(
    X,
    y_preds_reshaped[:, indices],
    color='black',
    alpha=0.2,
    label='Individual Posterior Regression Lines'
)


plt.xlabel('X')
plt.ylabel('y')
plt.title('Bayesian Linear Regression with Uncertainty')
# Fixing legend to avoid duplicate labels
handles, labels = plt.gca().get_legend_handles_labels()
by_label = dict(zip(labels, handles))
plt.legend(by_label.values(), by_label.keys())

plt.show()